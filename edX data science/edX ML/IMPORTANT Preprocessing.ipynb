{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A Note on SKLearn .transform() calls:\n",
    "Any time you transform your data, you lose the column header names. This actually makes complete sense. There are essentially two types of transformations,  those that change the scale of your features, and those that change your features entire. **Changing the scale would be like changing centimeters to inches. Changing the features would be like using PCA to reduce 300 columns to 30.** In either case, the original column's units have been altered or no longer exist, so it's up to you to rename your columns after ANY transformation. Due to this, SKLearn returns an NDArray from *transform() calls."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " SKLearn has many different methods for doing transforming your features by scaling them (this is a type of pre-processing).\n",
    "    \n",
    "    RobustScaler, Normalizer, MinMaxScaler, MaxAbsScaler, StandardScaler...\n",
    "    http://scikit-learn.org/stable/modules/classes.html module-sklearn.preprocessing\n",
    "   \n",
    "However in order to be effective at PCA, there are a few requirements that must be met, and which will drive the selection of your scaler. PCA required your data is standardized -- in other words it's mean is equal to 0, and it has ~unit variance.\n",
    "   \n",
    "    SKLearn's regular Normalizer doesn't zero out the mean of your data,\n",
    "    it only clamps it, so it's inappropriate to use here (depending on\n",
    "    your data). MinMaxScaler and MaxAbsScaler both fail to set a unit\n",
    "    variance, so you won't be using them either. RobustScaler can work,\n",
    "    again depending on your data (watch for outliers). For these reasons\n",
    "    we're going to use the StandardScaler. Get familiar with it by visiting\n",
    "    these two websites:\n",
    "   \n",
    "    http://scikit-learn.org/stable/modules/preprocessing.html preprocessing-scaler\n",
    "\n",
    "http://scikitlearn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html \n",
    "\n",
    "sklearn.preprocessing.StandardScaler\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# A Note on SKLearn .transform() calls:\n",
    "#\n",
    "# Any time you transform your data, you lose the column header names.\n",
    "# This actually makes complete sense. There are essentially two types\n",
    "# of transformations,  those that change the scale of your features,\n",
    "# and those that change your features entire. Changing the scale would\n",
    "# be like changing centimeters to inches. Changing the features would\n",
    "# be like using PCA to reduce 300 columns to 30. In either case, the\n",
    "# original column's units have been altered or no longer exist, so it's\n",
    "# up to you to rename your columns after ANY transformation. Due to\n",
    "# this, SKLearn returns an NDArray from *transform() calls.\n",
    "\n",
    "def scaleFeatures(df):\n",
    "  # SKLearn has many different methods for doing transforming your\n",
    "  # features by scaling them (this is a type of pre-processing).\n",
    "  # RobustScaler, Normalizer, MinMaxScaler, MaxAbsScaler, StandardScaler...\n",
    "  # http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "  #\n",
    "  # However in order to be effective at PCA, there are a few requirements\n",
    "  # that must be met, and which will drive the selection of your scaler.\n",
    "  # PCA required your data is standardized -- in other words it's mean is\n",
    "  # equal to 0, and it has ~unit variance.\n",
    "  #\n",
    "  # SKLearn's regular Normalizer doesn't zero out the mean of your data,\n",
    "  # it only clamps it, so it's inappropriate to use here (depending on\n",
    "  # your data). MinMaxScaler and MaxAbsScaler both fail to set a unit\n",
    "  # variance, so you won't be using them either. RobustScaler can work,\n",
    "  # again depending on your data (watch for outliers). For these reasons\n",
    "  # we're going to use the StandardScaler. Get familiar with it by visiting\n",
    "  # these two websites:\n",
    "  #\n",
    "  # http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "  #\n",
    "  # http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\n",
    "  #\n",
    "\n",
    "\n",
    "  # ---------\n",
    "  # Feature scaling is the type of transformation that only changes the\n",
    "  # scale and not number of features, so we'll use the original dataset\n",
    "  # column names. However we'll keep in mind that the _units_ have been\n",
    "  # altered:\n",
    "  scaled = preprocessing.StandardScaler().fit_transform(df)\n",
    "  scaled = pd.DataFrame(scaled, columns=df.columns)\n",
    "  print (\"New Variances:\\n\", scaled.var())\n",
    "  print (\"New Describe:\\n\", scaled.describe())\n",
    "  return scaled\n",
    "\n",
    "\n",
    "def drawVectors(transformed_features, components_, columns, plt, scaled):\n",
    "  if not scaled:\n",
    "    return plt.axes() # No cheating ;-)\n",
    "\n",
    "  num_columns = len(columns)\n",
    "\n",
    "  # This funtion will project your *original* feature (columns)\n",
    "  # onto your principal component feature-space, so that you can\n",
    "  # visualize how \"important\" each one was in the\n",
    "  # multi-dimensional scaling\n",
    "  \n",
    "  # Scale the principal components by the max value in\n",
    "  # the transformed set belonging to that component\n",
    "  xvector = components_[0] * max(transformed_features[:,0])\n",
    "  yvector = components_[1] * max(transformed_features[:,1])\n",
    "\n",
    "  ## visualize projections\n",
    "\n",
    "  # Sort each column by it's length. These are your *original*\n",
    "  # columns, not the principal components.\n",
    "  important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) }\n",
    "  important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)\n",
    "  print (\"Features by importance:\\n\", important_features)\n",
    "\n",
    "  ax = plt.axes()\n",
    "\n",
    "  for i in range(num_columns):\n",
    "    # Use an arrow to project each original feature as a\n",
    "    # labeled vector on your principal component axes\n",
    "    plt.arrow(0, 0, xvector[i], yvector[i], color='b', width=0.0005, head_width=0.02, alpha=0.75)\n",
    "    plt.text(xvector[i]*1.2, yvector[i]*1.2, list(columns)[i], color='b', alpha=0.75)\n",
    "\n",
    "  return ax\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
